{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e75223ec-eb5d-45f1-8de1-6a6efb7db114",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use kaggle api to download glove embeddings\n",
    "# Generate kaggle api key\n",
    "# https://www.kaggle.com/docs/api#getting-started-installation-&-authentication\n",
    "# Run `kaggle datasets download -d anmolkumar/glove-embeddings`\n",
    "# glove-embeddings.zip will be created\n",
    "# mkdir -p glove.6B\n",
    "# unzip -qq glove-embeddings.zip -d glove.6B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc940545-61fd-4640-99b3-35fbc3ecf843",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !conda install -c anaconda bcolz\n",
    "# !pip install spacy\n",
    "# python -m spacy download en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "35535289-ea73-4b6b-b036-6d558e1eb659",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !mkdir -p raw_data\n",
    "# !mkdir -p data\n",
    "# !wget -O raw_data/Flickr8k_Dataset.zip \"https://github.com/jbrownlee/Datasets/releases/download/Flickr8k/Flickr8k_Dataset.zip\"\n",
    "# !wget -O raw_data/Flickr8k_text.zip \"https://github.com/jbrownlee/Datasets/releases/download/Flickr8k/Flickr8k_text.zip\"\n",
    "# !unzip -qq raw_data/Flickr8k_Dataset.zip -d data/\n",
    "# !unzip -qq raw_data/Flickr8k_text.zip -d data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "970a997a-57c0-46fd-aee6-a604428ea020",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pickle\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "from tqdm import tqdm\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aedae690-4b3c-47c7-8320-0d806e90ae87",
   "metadata": {},
   "outputs": [],
   "source": [
    "from generate_vocab import Vocabulary\n",
    "from dataset import get_data_loader\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "725add41-52f1-4717-a22e-5cb42eb6bed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "36ace265-ab4e-4ff0-a614-af61172722e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/vocab.pkl', 'rb') as f:\n",
    "    vocab = pickle.load(f)\n",
    "\n",
    "glove_vectors = pickle.load(open('glove.6B/glove_words.pkl', 'rb'))\n",
    "glove_vectors = torch.tensor(glove_vectors)\n",
    "\n",
    "vocab_size = len(vocab)\n",
    "embed_size = glove_vectors.size()[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "83f46511-cf02-4973-8f9f-ea5551acaaaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class EncoderCNN(nn.Module):\n",
    "#     def __init__(self, embed_size):\n",
    "#         super(EncoderCNN, self).__init__()\n",
    "#         resnet = models.resnet101(pretrained=True)\n",
    "#         self.resnet = nn.Sequential(*list(resnet.children())[:-1])\n",
    "#         self.fc = nn.Linear(2048, embed_size)\n",
    "#         self.relu = nn.ReLU()\n",
    "\n",
    "#     def forward(self, images):\n",
    "#         out = self.resnet(images)\n",
    "#         out = out.view(1, -1)\n",
    "#         out = self.relu(self.fc(out))\n",
    "#         return out\n",
    "\n",
    "class EncoderCNN(nn.Module):\n",
    "    def __init__(self, embed_size, train_CNN=False):\n",
    "        super(EncoderCNN, self).__init__()\n",
    "        self.train_CNN = train_CNN\n",
    "        self.inception = models.inception_v3(pretrained=True, aux_logits=False)\n",
    "        self.inception.fc = nn.Linear(self.inception.fc.in_features, embed_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, images):\n",
    "        features = self.inception(images)\n",
    "        return self.dropout(self.relu(features))\n",
    "    \n",
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, embed_size, hidden_size, vocab_size, num_layers):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
    "        self.embed.weight = nn.Parameter(glove_vectors)\n",
    "        for p in self.embed.parameters():\n",
    "            p.requires_grad = True\n",
    "        \n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers)\n",
    "        self.linear = nn.Linear(hidden_size, vocab_size)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "\n",
    "    def forward(self, features, captions):\n",
    "        embeddings = self.dropout(self.embed(captions))\n",
    "        embeddings = torch.cat((features.unsqueeze(0), embeddings), dim=0).float()\n",
    "        hiddens, _ = self.lstm(embeddings)\n",
    "        outputs = self.linear(hiddens)\n",
    "        return outputs\n",
    "\n",
    "class CNNtoRNN(nn.Module):\n",
    "    def __init__(self, embed_size, hidden_size, vocab_size, num_layers):\n",
    "        super(CNNtoRNN, self).__init__()\n",
    "        self.encoderCNN = EncoderCNN(embed_size)\n",
    "        self.decoderRNN = DecoderRNN(embed_size, hidden_size, vocab_size, num_layers)\n",
    "\n",
    "    def forward(self, images, captions):\n",
    "        features = self.encoderCNN(images)\n",
    "        outputs = self.decoderRNN(features, captions)\n",
    "        return outputs\n",
    "\n",
    "    def caption_image(self, image, vocab, max_length=50):\n",
    "        result_caption = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            x = self.encoderCNN(image).unsqueeze(0)\n",
    "            states = None\n",
    "\n",
    "            for _ in range(max_length):\n",
    "                hiddens, states = self.decoderRNN.lstm(x.float(), states)\n",
    "                output = self.decoderRNN.linear(hiddens.squeeze(0))\n",
    "                predicted = output.argmax(1)\n",
    "                result_caption.append(predicted.item())\n",
    "                x = self.decoderRNN.embed(predicted).unsqueeze(0)\n",
    "\n",
    "                if vocab.idx2word[predicted.item()] == \"<eos>\":\n",
    "                    break\n",
    "\n",
    "        return [vocab.idx2word[idx] for idx in result_caption]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "df10129c-c633-4f71-9302-0885491a0ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    def __init__(self):\n",
    "        self.avg = 0.\n",
    "        self.sum = 0.\n",
    "        self.count = 0.\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "        \n",
    "# class EarlyStopping:\n",
    "#     def __init__(self, patience=7, mode=\"max\", delta=0.001):\n",
    "#         self.patience = patience\n",
    "#         self.counter = 0\n",
    "#         self.mode = mode\n",
    "#         self.best_score = None\n",
    "#         self.best_epoch = 0\n",
    "#         self.early_stop = False\n",
    "#         self.delta = delta\n",
    "#         if self.mode == \"min\":\n",
    "#             self.val_score = np.Inf\n",
    "#         else:\n",
    "#             self.val_score = -np.Inf\n",
    "\n",
    "#     def __call__(self, epoch, epoch_score, model, model_path):\n",
    "\n",
    "#         if self.mode == \"min\":\n",
    "#             score = -1.0 * epoch_score\n",
    "#         else:\n",
    "#             score = np.copy(epoch_score)\n",
    "\n",
    "#         if self.best_score is None:\n",
    "#             self.best_score = score\n",
    "#             self.best_epoch = epoch\n",
    "#             self.save_checkpoint(epoch_score, model, model_path)\n",
    "#         elif score < self.best_score + self.delta:\n",
    "#             self.counter += 1\n",
    "#             print(f\"EarlyStopping counter: {self.counter} out of {self.patience}\")\n",
    "#             if self.counter >= self.patience:\n",
    "#                 self.early_stop = True\n",
    "#         else:\n",
    "#             self.best_score = score\n",
    "#             self.best_epoch = epoch\n",
    "#             self.save_checkpoint(epoch_score, model, model_path)\n",
    "#             self.counter = 0\n",
    "\n",
    "#     def save_checkpoint(self, epoch_score, model, model_path):\n",
    "#         if epoch_score not in [-np.inf, np.inf, -np.nan, np.nan]:\n",
    "#             print(f\"Validation score improved ({self.val_score} --> {epoch_score}). Saving model!\")\n",
    "#             torch.save(model.state_dict(), model_path)\n",
    "#         self.val_score = epoch_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2a2e5c2d-3411-4eba-81fe-e5596fb0c142",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, data_loader):\n",
    "    model.train()\n",
    "    for idx, (image_filenames, image_nums, text_captions, images, captions) in tqdm(\n",
    "        enumerate(train_dataloader), total=len(train_dataloader), leave=False):\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        images = images.to(device)\n",
    "        captions = captions.to(device)\n",
    "\n",
    "        outputs = model(images, captions[:-1])\n",
    "        loss = criterion(\n",
    "            outputs.reshape(-1, outputs.shape[2]), captions.reshape(-1)\n",
    "        )\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "def evaluate(model, optimizer, data_loader):\n",
    "    model.eval()\n",
    "    avg_meter = AverageMeter()\n",
    "    predicted_captions = []\n",
    "    actual_captions = []\n",
    "    for idx, (image_filenames, image_nums, text_captions, images, captions) in tqdm(\n",
    "        enumerate(train_dataloader), total=len(train_dataloader), leave=False):\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        images = images.to(device)\n",
    "        captions = captions.to(device)\n",
    "\n",
    "        outputs = model(images, captions[:-1])\n",
    "        loss = criterion(\n",
    "            outputs.reshape(-1, outputs.shape[2]), captions.reshape(-1)\n",
    "        )\n",
    "        \n",
    "    #     for image, caption in zip(images, captions):\n",
    "    #         preds = model.caption_image(image.unsqueeze(0), vocab)\n",
    "    #         predicted_captions.append(preds)\n",
    "    #         actual_captions.append([vocab.idx2word[i.item()] for i in caption][1:-1])\n",
    "    # bleu_score = bleu_score(actual_captions, predicted_captions)\n",
    "    # return bleu_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "014b85b4-de48-41cd-8521-d719dd7087e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    # transforms.RandomCrop(224),\n",
    "    # transforms.RandomHorizontalFlip(),\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.485, 0.456, 0.406),\n",
    "                        (0.229, 0.224, 0.225))])\n",
    "train_dataloader = get_data_loader(data_type=\"train\", vocab=vocab, transforms=transform, num_workers=4)\n",
    "dev_dataloader = get_data_loader(data_type=\"dev\", vocab=vocab, transforms=transform, num_workers=4)\n",
    "test_dataloader = get_data_loader(data_type=\"test\", vocab=vocab, transforms=transform, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "09b7705a-b709-4641-9060-8744fa16fd91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [9]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     14\u001b[0m train(model, optimizer, train_dataloader)\n\u001b[0;32m---> 15\u001b[0m \u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdev_dataloader\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [7]\u001b[0m, in \u001b[0;36mevaluate\u001b[0;34m(model, optimizer, data_loader)\u001b[0m\n\u001b[1;32m     23\u001b[0m predicted_captions \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     24\u001b[0m actual_captions \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 25\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx, (image_filenames, image_nums, text_captions, images, captions) \u001b[38;5;129;01min\u001b[39;00m tqdm(\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28menumerate\u001b[39m(train_dataloader), total\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(train_dataloader), leave\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m     28\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     30\u001b[0m     images \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/tqdm/std.py:1180\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1177\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1179\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1180\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1181\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m   1182\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1183\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/utils/data/dataloader.py:530\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    528\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    529\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()\n\u001b[0;32m--> 530\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    531\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    532\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    533\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    534\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/utils/data/dataloader.py:1195\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1192\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_data(data)\n\u001b[1;32m   1194\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m-> 1195\u001b[0m idx, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1196\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1197\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable:\n\u001b[1;32m   1198\u001b[0m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/utils/data/dataloader.py:1151\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1149\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m   1150\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_thread\u001b[38;5;241m.\u001b[39mis_alive():\n\u001b[0;32m-> 1151\u001b[0m         success, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_try_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1152\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[1;32m   1153\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/utils/data/dataloader.py:999\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    986\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_try_get_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m_utils\u001b[38;5;241m.\u001b[39mMP_STATUS_CHECK_INTERVAL):\n\u001b[1;32m    987\u001b[0m     \u001b[38;5;66;03m# Tries to fetch data from `self._data_queue` once for a given timeout.\u001b[39;00m\n\u001b[1;32m    988\u001b[0m     \u001b[38;5;66;03m# This can also be used as inner loop of fetching without timeout, with\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    996\u001b[0m     \u001b[38;5;66;03m# Returns a 2-tuple:\u001b[39;00m\n\u001b[1;32m    997\u001b[0m     \u001b[38;5;66;03m#   (bool: whether successfully get data, any: data if successful else None)\u001b[39;00m\n\u001b[1;32m    998\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 999\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_queue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1000\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n\u001b[1;32m   1001\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1002\u001b[0m         \u001b[38;5;66;03m# At timeout and error, we manually check whether any worker has\u001b[39;00m\n\u001b[1;32m   1003\u001b[0m         \u001b[38;5;66;03m# failed. Note that this is the only mechanism for Windows to detect\u001b[39;00m\n\u001b[1;32m   1004\u001b[0m         \u001b[38;5;66;03m# worker failures.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/queue.py:179\u001b[0m, in \u001b[0;36mQueue.get\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    177\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m remaining \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m:\n\u001b[1;32m    178\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[0;32m--> 179\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnot_empty\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mremaining\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get()\n\u001b[1;32m    181\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnot_full\u001b[38;5;241m.\u001b[39mnotify()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/threading.py:306\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    304\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    305\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 306\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    307\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    308\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m waiter\u001b[38;5;241m.\u001b[39macquire(\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "NUM_EPOCHS = 2\n",
    "seed_everything(2022)\n",
    "model = CNNtoRNN(embed_size, 256, vocab_size, 1).to(device)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "for name, param in model.encoderCNN.inception.named_parameters():\n",
    "    if \"fc.weight\" in name or \"fc.bias\" in name:\n",
    "        param.requires_grad = True\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    print(f\"Epoch: {epoch + 1}\")\n",
    "    train(model, optimizer, train_dataloader)\n",
    "    evaluate(model, optimizer, dev_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a35cecb8-1f0a-47cc-8e7c-f50efeda76d1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
